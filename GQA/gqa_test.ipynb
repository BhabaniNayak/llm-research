{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto reload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.base import CustomPreTrainedModel\n",
    "from src.utils import measure_inference_speed, measure_memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input text\n",
    "input_text = \"This is a sample input text to measure the inference speed of the model.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, AutoConfig, BertModel, RobertaModel, DistilBertModel, AlbertModel, GPT2Model, T5Model, XLNetModel, ElectraModel, BartModel, DebertaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_classes = {\n",
    "    \"bert-base-uncased\": BertModel,\n",
    "    \"bert-large-uncased\": BertModel,\n",
    "    \"roberta-base\": RobertaModel,\n",
    "    # \"roberta-large\": RobertaModel,\n",
    "    # \"distilbert-base-uncased\": DistilBertModel,\n",
    "    # \"albert-base-v2\": AlbertModel,\n",
    "    # \"albert-large-v2\": AlbertModel,\n",
    "    # \"gpt2\": GPT2Model,\n",
    "    # \"gpt2-medium\": GPT2Model,\n",
    "    # \"t5-small\": T5Model,\n",
    "    # \"t5-base\": T5Model,\n",
    "    # \"t5-large\": T5Model,\n",
    "    # \"xlnet-base-cased\": XLNetModel,\n",
    "    # \"xlnet-large-cased\": XLNetModel,\n",
    "    # \"google/electra-small-discriminator\": ElectraModel,\n",
    "    # \"google/electra-base-discriminator\": ElectraModel,\n",
    "    # \"facebook/bart-base\": BartModel,\n",
    "    # \"facebook/bart-large\": BartModel,\n",
    "    # \"microsoft/deberta-base\": DebertaModel,\n",
    "    # \"microsoft/deberta-large\": DebertaModel\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model: bert-base-uncased\n",
      "Inference speed: 0.076752 seconds\n",
      "Custom model inference speed: 0.026537 seconds\n",
      "Memory usage: 1420.90 MB\n",
      "Custom Memory usage: 1421.39 MB\n",
      "Speedup: 2.89x\n",
      "% difference in memory usage: -0.03%\n",
      "****************************************************************************************************\n",
      "Testing model: bert-large-uncased\n",
      "Inference speed: 0.082815 seconds\n",
      "Custom model inference speed: 0.080833 seconds\n",
      "Memory usage: 4791.22 MB\n",
      "Custom Memory usage: 4791.73 MB\n",
      "Speedup: 1.02x\n",
      "% difference in memory usage: -0.01%\n",
      "****************************************************************************************************\n",
      "Testing model: roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference speed: 0.033722 seconds\n",
      "Custom model inference speed: 0.031844 seconds\n",
      "Memory usage: 3821.50 MB\n",
      "Custom Memory usage: 3821.71 MB\n",
      "Speedup: 1.06x\n",
      "% difference in memory usage: -0.01%\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "for model_name in model_classes:\n",
    "    print(f\"Testing model: {model_name}\")\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    \n",
    "    custom_model = CustomPreTrainedModel(config, model_classes[model_name])\n",
    "\n",
    "    speed = measure_inference_speed(model, tokenizer, input_text)\n",
    "    memory = measure_memory_usage(model, tokenizer, input_text)\n",
    "    \n",
    "    custom_speed = measure_inference_speed(custom_model, tokenizer, input_text)\n",
    "    custom_memory = measure_memory_usage(custom_model, tokenizer, input_text)\n",
    "\n",
    "    print(f\"Inference speed: {speed:.6f} seconds\")\n",
    "    print(f\"Custom model inference speed: {custom_speed:.6f} seconds\")\n",
    "        \n",
    "    print(f\"Memory usage: {memory / (1024 ** 2):.2f} MB\")\n",
    "    print(f\"Custom Memory usage: {custom_memory / (1024 ** 2):.2f} MB\")\n",
    "\n",
    "    print(f\"Speedup: {speed / custom_speed:.2f}x\")\n",
    "    print(f\"% difference in memory usage: {((memory - custom_memory) / memory) * 100:.2f}%\")\n",
    "    \n",
    "    print(\"*\" * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "be",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

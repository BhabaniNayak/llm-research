{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import psutil\n",
    "from transformers import BertModel, BertTokenizer, BertConfig\n",
    "from transformers.models.bert.modeling_bert import BertSelfAttention\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPastAndCrossAttentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom BertSelfAttention to simulate Grouped Query Attention\n",
    "class CustomBertSelfAttention(BertSelfAttention):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        # Override the number of attention heads for GQA simulation\n",
    "        self.num_attention_heads = 8 # Drastically reduce heads to 2 for clear difference. You can still use 8 for noticeable speed difference\n",
    "        \n",
    "        self.attention_head_size = int(self.all_head_size / self.num_attention_heads)\n",
    "        \n",
    "        self.query = torch.nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = torch.nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = torch.nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(config.attention_probs_dropout_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom BertLayer to replace the self-attention with CustomBertSelfAttention\n",
    "class CustomBertLayer(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = CustomBertSelfAttention(config)\n",
    "        self.intermediate = torch.nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.output = torch.nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.LayerNorm = torch.nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        hidden_states, \n",
    "        attention_mask=None, \n",
    "        head_mask=None, \n",
    "        encoder_hidden_states=None, \n",
    "        encoder_attention_mask=None, \n",
    "        past_key_value=None, \n",
    "        output_attentions=False):\n",
    "            \n",
    "        attention_output = self.attention(hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\n",
    "        \n",
    "        intermediate_output = self.intermediate(attention_output[0])\n",
    "        \n",
    "        layer_output = self.output(intermediate_output)\n",
    "        layer_output = self.dropout(layer_output)\n",
    "        layer_output = self.LayerNorm(layer_output + attention_output[0])\n",
    "        \n",
    "        return (layer_output,) + attention_output[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom BertEncoder to use the CustomBertLayer\n",
    "class CustomBertEncoder(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layer = torch.nn.ModuleList([CustomBertLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            hidden_states, \n",
    "            attention_mask=None, \n",
    "            head_mask=None, \n",
    "            encoder_hidden_states=None, \n",
    "            encoder_attention_mask=None, \n",
    "            past_key_values=None, \n",
    "            use_cache=None, \n",
    "            output_attentions=False, \n",
    "            output_hidden_states=False, \n",
    "            return_dict=True):\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_attentions = () if output_attentions else None\n",
    "        next_decoder_cache = () if use_cache else None\n",
    "\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            layer_outputs = layer_module(\n",
    "                hidden_states,\n",
    "                attention_mask,\n",
    "                head_mask[i] if head_mask is not None else None,\n",
    "                encoder_hidden_states,\n",
    "                encoder_attention_mask,\n",
    "                past_key_values[i] if past_key_values is not None else None,\n",
    "                output_attentions,\n",
    "            )\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            if use_cache:\n",
    "                next_decoder_cache += (layer_outputs[1],)\n",
    "\n",
    "            if output_attentions:\n",
    "                all_attentions = all_attentions + (layer_outputs[1],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [hidden_states, next_decoder_cache, all_hidden_states, all_attentions]\n",
    "                if v is not None\n",
    "            )\n",
    "        return BaseModelOutputWithPastAndCrossAttentions(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_decoder_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom BERT model using the CustomBertEncoder\n",
    "class CustomBertModel(BertModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.encoder = CustomBertEncoder(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to measure inference speed\n",
    "def measure_inference_speed(model, tokenizer, input_text, num_runs=10):\n",
    "    inputs = tokenizer(input_text, return_tensors='pt')\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "        for _ in range(num_runs):\n",
    "            outputs = model(**inputs)\n",
    "        end_time = time.time()\n",
    "    avg_time = (end_time - start_time) / num_runs\n",
    "    return avg_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare memory usage\n",
    "def measure_memory_usage(model, tokenizer, input_text):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(input_text, return_tensors='pt')\n",
    "    \n",
    "    # Move inputs to the same device as the model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    model.to(device)\n",
    "    \n",
    "    # Run the model to ensure it's loaded into memory\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        memory_usage = torch.cuda.memory_allocated()\n",
    "    else:\n",
    "        process = psutil.Process()\n",
    "        memory_usage = process.memory_info().rss  # in bytes\n",
    "\n",
    "    return memory_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Execution Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the custom BERT model with simulated GQA\n",
    "config = BertConfig.from_pretrained(model_name)\n",
    "custom_model = CustomBertModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input text\n",
    "input_text = \"This is a sample input text to measure the inference speed of the model.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base BERT model inference speed: 0.081608 seconds\n",
      "GQA BERT model inference speed: 0.022804 seconds\n",
      "Speedup: 3.58x\n"
     ]
    }
   ],
   "source": [
    "# Measure the inference speed of all models\n",
    "base_model_speed = measure_inference_speed(model, tokenizer, input_text)\n",
    "gqa_model_speed = measure_inference_speed(custom_model, tokenizer, input_text)\n",
    "\n",
    "print(f\"Base BERT model inference speed: {base_model_speed:.6f} seconds\")\n",
    "print(f\"GQA BERT model inference speed: {gqa_model_speed:.6f} seconds\")\n",
    "print(f\"Speedup: {base_model_speed / gqa_model_speed:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base BERT Model Memory Usage: 1497899008 bytes\n",
      "GQA BERT Model Memory Usage: 1497903104 bytes\n",
      "% difference in memory usage: -0.00%\n"
     ]
    }
   ],
   "source": [
    "# Measure memory usage for both models\n",
    "base_model_memory = measure_memory_usage(model, tokenizer, input_text)\n",
    "gqa_model_memory = measure_memory_usage(custom_model, tokenizer, input_text)\n",
    "memory_diff = base_model_memory - gqa_model_memory\n",
    "\n",
    "# Print the results\n",
    "print(f\"Base BERT Model Memory Usage: {base_model_memory} bytes\")\n",
    "print(f\"GQA BERT Model Memory Usage: {gqa_model_memory} bytes\")\n",
    "print(f\"% difference in memory usage: {(memory_diff / base_model_memory) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- The differences in memory usage may be negligible when running on a CPU. \n",
    "    - The memory savings and performance improvements of Grouped Query Attention (GQA) are more noticeable in GPU environments due to the higher parallelism and larger memory capacities involved.\n",
    "- Larger Model: Switching to a larger model (bert-large-uncased) will make the difference in computational requirements more pronounced.\n",
    "- Longer Input Text: Using a longer input text will highlight differences in execution time more clearly.\n",
    "- Inference Speed Focus: Measuring inference speed will likely show a more noticeable difference on the CPU compared to memory usage."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "be",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
